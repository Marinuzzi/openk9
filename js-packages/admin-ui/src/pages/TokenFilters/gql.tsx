import { gql } from "@apollo/client";
import { TemplateType } from "@pages/Analyzer/gql";

gql`
  query TokenFilters($searchText: String, $after: String) {
    tokenFilters(searchText: $searchText, first: 20, after: $after) {
      edges {
        node {
          id
          name
          description
        }
      }
      pageInfo {
        hasNextPage
        endCursor
      }
    }
  }
`;

gql`
  mutation DeleteTokenFilters($id: ID!) {
    deleteTokenFilter(tokenFilterId: $id) {
      id
      name
    }
  }
`;

gql`
  mutation AddTokenFilterToAnalyzer($childId: ID!, $parentId: ID!) {
    addTokenFilterToAnalyzer(tokenFilterId: $childId, id: $parentId) {
      left {
        id
      }
      right {
        id
      }
    }
  }
`;

gql`
  query UnboundAnalyzersByTokenFilter($tokenFilterId: BigInteger!) {
    unboundAnalyzersByTokenFilter(tokenFilterId: $tokenFilterId) {
      name
      id
    }
  }
`;

gql`
  query TokenFilter($id: ID!) {
    tokenFilter(id: $id) {
      id
      name
      description
      jsonConfig
      type
    }
  }
`;

gql`
  mutation CreateOrUpdateTokenFilter(
    $id: ID
    $name: String!
    $description: String
    $jsonConfig: String
    $type: String!
  ) {
    tokenFilter(
      id: $id
      tokenFilterDTO: { name: $name, description: $description, jsonConfig: $jsonConfig, type: $type }
    ) {
      entity {
        id
        name
      }
      fieldValidators {
        field
        message
      }
    }
  }
`;

export const Filters: TemplateType[] = [
  {
    title: "apostrophe",
    description: "Strips all characters after an apostrophe, including the apostrophe itself.",
    type: "apostrophe",
    value: [],
  },
  {
    title: "classic",
    description: "Performs optional post-processing of terms generated by the classic tokenizer.",
    type: "classic",
    value: [],
  },
  {
    title: "trim",
    description:
      "Removes leading and trailing whitespace from each token in a stream. While this can change the length of a token, the trim filter does not change a token’s offsets.",
    type: "trim",
    value: [],
  },
  {
    title: "uppercase",
    description:
      "Changes token text to uppercase. For example, you can use the uppercase filter to change the Lazy DoG to THE LAZY DOG.",
    type: "uppercase",
    value: [],
  },
  {
    title: "reverse",
    description: "Reverses each token in a stream. For example, you can use the reverse filter to change cat to tac.",
    type: "reverse",
    value: [],
  },
  {
    title: "remove_duplicates",
    description: "Removes duplicate tokens in the same position.",
    type: "remove_duplicates",
    value: [],
  },
  {
    title: "edge_ngram",
    description: "Forms an n-gram of a specified length from the beginning of a token.",
    type: "edge_ngram",
    value: [
      { name: "min_gram", value: 1, type: "number", description: "Minimum character length of a gram. Defaults to 1." },
      {
        name: "max_gram",
        value: 2,
        type: "number",
        description:
          "Maximum character length of a gram. For custom token filters, defaults to 2. For the built-in edge_ngram filter, defaults to 1.",
      },
      {
        name: "side",
        value: "front",
        type: "string",
        description: "Indicates whether to truncate tokens from the front or back. Defaults to front",
      },
      {
        name: "preserve_original",
        value: false,
        type: "boolean",
        description: "Emits original token when set to true. Defaults to false.",
      },
    ],
  },
  {
    title: "stop",
    description: "Removes stop words from a token stream.",
    type: "stop",
    value: [
      {
        name: "stopwords",
        value: ["a", "e", "i", "o", "u"],
        type: "multi-select",
        description:
          "(Optional, string or array of strings) Language value, such as _arabic_ or _thai_. Defaults to _english_.",
      },
      {
        name: "ignore_case",
        value: false,
        type: "boolean",
        description:
          "If true, stop word matching is case insensitive. For example, if true, a stop word of the matches and removes The, THE, or the. Defaults to false.",
      },
      {
        name: "remove_trailing",
        value: true,
        type: "boolean",
        description: "If true, the last token of a stream is removed if it s a stop word. Defaults to true.",
      },
    ],
  },
  {
    title: "ngram",
    description: "Forms n-grams of specified lengths from a token.",
    type: "ngram",
    value: [
      { name: "min_gram", value: 3, type: "number", description: "Minimum character length of a gram. Defaults to 1." },
      {
        name: "max_gram",
        value: 5,
        type: "number",
        description:
          "Maximum character length of a gram. For custom token filters, defaults to 2. For the built-in edge_ngram filter, defaults to 1.",
      },
      {
        name: "preserve_original",
        value: false,
        type: "boolean",
        description: "Emits original token when set to true. Defaults to false.",
      },
    ],
  },
  {
    title: "shingle",
    description:
      "Add shingles, or word n-grams, to a token stream by concatenating adjacent tokens. By default, the shingle token filter outputs two-word shingles and unigrams.",
    type: "shingle",
    value: [
      {
        name: "min_shingle_size",
        value: 2,
        type: "number",
        description: "Minimum number of tokens to concatenate when creating shingles. Defaults to 2.",
      },
      {
        name: "max_shingle_size",
        value: 3,
        type: "number",
        description: "Maximum number of tokens to concatenate when creating shingles. Defaults to 2.",
      },
      {
        name: "output_unigrams",
        value: true,
        type: "boolean",
        description:
          "f true, the output includes the original input tokens. If false, the output only includes shingles; the original input tokens are removed. Defaults to true.",
      },
      {
        name: "output_unigrams_if_no_shingles",
        value: false,
        type: "boolean",
        description: "Whether to output unigrams if no shingles are generated. Default is false.",
      },
      {
        name: "token_separator",
        value: "",
        type: "string",
        description: "A separator used to concatenate tokens into a shingle. Default is a space",
      },
      {
        name: "filler_token",
        value: "_",
        type: "string",
        description: "A token inserted into empty positions or gaps between tokens. Default is an underscore (_).",
      },
    ],
  },
  {
    title: "snowball",
    description: "A filter that stems words using a Snowball-generated stemmer.",
    type: "snowball",
    value: [{ name: "language", value: "English", type: "string" }],
  },
  {
    title: "stemmer",
    description: "Provides algorithmic stemming for several languages, some with additional variants.",
    type: "stemmer",
    value: [{ name: "language", value: "light_german", type: "string", description: "Language" }],
  },
  {
    title: "porter_stem",
    description: "Provides algorithmic stemming for the English language, based on the Porter stemming algorithm.",
    type: "porter_stem",
    value: [],
  },
  {
    title: "truncate",
    description:
      "Truncates tokens that exceed a specified character limit. This limit defaults to 10 but can be customized using the length parameter.",
    type: "truncate",
    value: [
      {
        name: "length",
        value: 10,
        type: "number",
        description:
          "Truncates tokens that exceed a specified character limit. This limit defaults to 10 but can be customized using the length parameter.",
      },
    ],
  },
  {
    title: "unique",
    description: "Removes duplicate tokens in the same position.",
    type: "unique",
    value: [
      {
        name: "only_on_same_position",
        value: true,
        type: "boolean",
        description: "If true, only remove duplicate tokens in the same position. Defaults to false.",
      },
    ],
  },
  {
    title: "pattern_replace",
    description: "Uses a regular expression to match and replace token substrings.",
    type: "pattern_replace",
    value: [
      {
        name: "pattern",
        value: "",
        type: "string",
        description:
          "Regular expression, written in Java’s regular expression syntax. The filter replaces token substrings matching this pattern with the substring in the replacement parameter.",
      },
      {
        name: "replacement",
        value: "",
        type: "string",
        description: "Replacement substring. Defaults to an empty substring.",
      },
      {
        name: "all",
        value: true,
        type: "boolean",
        description:
          "If true, all substrings matching the pattern parameter’s regular expression are replaced. If false, the filter replaces only the first matching substring in each token. Defaults to true.",
      },
    ],
  },
  {
    title: "limit",
    description:
      "Limits the number of output tokens. The limit filter is commonly used to limit the size of document field values based on token count.",
    type: "limit",
    value: [
      {
        name: "max_token_count",
        value: 10,
        type: "number",
        description:
          "Regular expression, written in Java’s regular expression syntax. The filter replaces token substrings matching this pattern with the substring in the replacement parameter.",
      },
      { name: "consume_all_tokens", value: false, type: "boolean" },
    ],
  },
  {
    title: "lowercase",
    description:
      "Changes token text to lowercase. For example, you can use the lowercase filter to change THE Lazy DoG to the lazy dog.",
    type: "lowercase",
    value: [{ name: "language", value: "", type: "string", description: "Language" }],
  },
  {
    title: "synonym",
    description:
      "The synonym token filter allows to easily handle synonyms during the analysis process. Synonyms are configured using a configuration file.",
    type: "synonym",
    value: [
      {
        name: "synonyms",
        value: [],
        type: "multi-select",
        description: "Synonyms based on WordNet format can be declared using format: s(100000001,1,'abstain',v,1,0).",
      },
      { name: "expand", value: true, type: "boolean", description: "Expand" },
      {
        name: "lenient",
        value: false,
        type: "boolean",
        description:
          "If true ignores exceptions while parsing the synonym configuration. It is important to note that only those synonym rules which cannot get parsed are ignored.",
      },
      {
        name: "format",
        value: "solr",
        type: "string",
        description:
          "Specifies the format used to determine how OpenSearch defines and interprets synonyms. Valid values are: - solr - wordnet. Default is solr.",
      },
    ],
  },
];
